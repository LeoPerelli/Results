{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MykPt6jzs6vF"
   },
   "source": [
    "# Install additional packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QNZes2-6s7EQ"
   },
   "outputs": [],
   "source": [
    "# # install custom packages - for google collab\n",
    "# !pip install datashader\n",
    "# !pip install hdbscan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mEgeIvRSs3EU"
   },
   "source": [
    "# Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 311,
     "status": "ok",
     "timestamp": 1599186611396,
     "user": {
      "displayName": "Bo Wen",
      "photoUrl": "",
      "userId": "04736056844233231284"
     },
     "user_tz": 420
    },
    "id": "OQWSxl2seaFb",
    "outputId": "970d01e1-bb46-44f7-cb22-3added8a0af6"
   },
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(\"python {}\".format(python_version()))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"pandas {}\".format(pd.__version__))\n",
    "print(\"numpy {}\".format(np.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "\n",
    "from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 42,
     "output_embedded_package_id": "1NYZvGH84SmKlBcAmPa9g122s0hlcDEcH"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8212,
     "status": "ok",
     "timestamp": 1599186631578,
     "user": {
      "displayName": "Bo Wen",
      "photoUrl": "",
      "userId": "04736056844233231284"
     },
     "user_tz": 420
    },
    "id": "JjKRJKMivHes",
    "outputId": "02497460-a02d-40e8-a543-b0b472505e28"
   },
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "import holoviews.operation.datashader as hd\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "\n",
    "hd.shade.cmap=[\"lightblue\", \"darkblue\"]\n",
    "hv.extension('bokeh', 'matplotlib') \n",
    "# https://datashader.org/getting_started/Interactivity.html\n",
    "\n",
    "# https://stackoverflow.com/questions/54793910/how-to-make-the-holoviews-show-graph-on-google-colaboratory-notebook\n",
    "%env HV_DOC_HTML=true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5TToUdDIXK4D"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1081,
     "status": "ok",
     "timestamp": 1599186641168,
     "user": {
      "displayName": "Bo Wen",
      "photoUrl": "",
      "userId": "04736056844233231284"
     },
     "user_tz": 420
    },
    "id": "5T9kvBlFisYs"
   },
   "outputs": [],
   "source": [
    "# set option to process raw data, False will read parsed data directly\n",
    "DATA_OPTION_PRCESS_RAW = False\n",
    "\n",
    "# set number of rows to work with\n",
    "DATA_OPTION_NUM_ROWS = 2307 # total row of data - 2307\n",
    "#DATA_OPTION_NUM_ROWS = None # all rows\n",
    "\n",
    "# set paths to data files\n",
    "RAW_DATA_FILE = 'raw_data/competition_dataset.csv'\n",
    "PARSED_DATA_FILE = 'intermediate_data/competition_dataset_long_{}.csv'.format(DATA_OPTION_NUM_ROWS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12469,
     "status": "ok",
     "timestamp": 1599186652776,
     "user": {
      "displayName": "Bo Wen",
      "photoUrl": "",
      "userId": "04736056844233231284"
     },
     "user_tz": 420
    },
    "id": "1L0MacMLEKhM"
   },
   "outputs": [],
   "source": [
    "if DATA_OPTION_PRCESS_RAW:\n",
    "\n",
    "    # read raw data to process into parsed data\n",
    "    raw_df = pd.read_csv(RAW_DATA_FILE, header=0, skiprows=0,\n",
    "                         nrows=DATA_OPTION_NUM_ROWS, delimiter=None)\n",
    "\n",
    "    parsed_df = raw_df.copy()\n",
    "    parsed_df['data'] = parsed_df.iloc[:, 0].str.split('; ')\n",
    "    parsed_df['count'] = parsed_df['data'].str.len()\n",
    "    parsed_df['count'] = (parsed_df['count'] - 4 - 1) / 6\n",
    "    parsed_df['count'] = parsed_df['count'].astype(int)\n",
    "\n",
    "    # credit: https://stackoverflow.com/a/59552714\n",
    "    spread_ixs = np.repeat(range(len(parsed_df)), parsed_df['count'])\n",
    "    # .drop(columns='count').reset_index(drop=True)\n",
    "    parsed_df = parsed_df.iloc[spread_ixs, :]\n",
    "\n",
    "    parsed_df['track_id'] = parsed_df['data'].str[0].astype(int)\n",
    "    parsed_df['grouped_row_id'] = parsed_df.groupby(\n",
    "        'track_id')['track_id'].rank(method='first').astype(int)\n",
    "\n",
    "    old_col = raw_df.columns.tolist()[0]\n",
    "    new_cols = old_col.split('; ')\n",
    "\n",
    "    # build columns\n",
    "    parsed_df['track_id'] = parsed_df['data'].apply(lambda x: x[0])\n",
    "    parsed_df['type'] = parsed_df['data'].apply(lambda x: x[1])\n",
    "    parsed_df['traveled_d'] = parsed_df['data'].apply(lambda x: x[2])\n",
    "    parsed_df['avg_speed'] = parsed_df['data'].apply(lambda x: x[3])\n",
    "\n",
    "    parsed_df['lat'] = parsed_df.apply(\n",
    "        lambda row: row['data'][4+(row['grouped_row_id']-1)*6], axis=1)\n",
    "    parsed_df['lon'] = parsed_df.apply(\n",
    "        lambda row: row['data'][5+(row['grouped_row_id']-1)*6], axis=1)\n",
    "    parsed_df['speed'] = parsed_df.apply(\n",
    "        lambda row: row['data'][6+(row['grouped_row_id']-1)*6], axis=1)\n",
    "    parsed_df['lon_acc'] = parsed_df.apply(\n",
    "        lambda row: row['data'][7+(row['grouped_row_id']-1)*6], axis=1)\n",
    "    parsed_df['lat_acc'] = parsed_df.apply(\n",
    "        lambda row: row['data'][8+(row['grouped_row_id']-1)*6], axis=1)\n",
    "    parsed_df['time'] = parsed_df.apply(\n",
    "        lambda row: row['data'][9+(row['grouped_row_id']-1)*6], axis=1)\n",
    "\n",
    "    # clean up columns\n",
    "    parsed_df = parsed_df.drop(columns=old_col)\n",
    "    parsed_df = parsed_df.drop(\n",
    "        columns=['count',\n",
    "                 'grouped_row_id',\n",
    "                 'data']\n",
    "    ).reset_index(drop=True)\n",
    "    parsed_df = parsed_df.reset_index(drop=False).rename(\n",
    "        columns={'index': 'record_id'})\n",
    "\n",
    "    # output to file\n",
    "    parsed_df.to_csv(PARSED_DATA_FILE, index=False)\n",
    "    parsed_df.head(5)\n",
    "\n",
    "else:\n",
    "    # read parsed data\n",
    "    parsed_df = pd.read_csv(PARSED_DATA_FILE, header=0,\n",
    "                            skiprows=0, delimiter=None)\n",
    "    parsed_df['track_id'] = parsed_df['track_id'].astype(int)\n",
    "\n",
    "# clean up unnamed index column - perhaps name it as record id?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute extra attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate orientation\n",
    "## bearing using acceleration (do not use as it provides inaccurate bearing)\n",
    "parsed_df['acc_angle'] = np.arctan2(parsed_df['lat_acc'],\n",
    "                                    parsed_df['lon_acc']) * 180 / np.pi  # lon = x, lat = y\n",
    "\n",
    "## approximate bearing using acceleration (do not use as it provides inaccurate bearing)\n",
    "parsed_df['appr_acc_angle'] = parsed_df['acc_angle'].round(-1)\n",
    "\n",
    "# https://stackoverflow.com/questions/1016039/determine-the-general-orientation-of-a-2d-vector\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.arctan2.html\n",
    "# np.arctan2(y, x) * 180 / np.pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute x and y corrdinates\n",
    "# this improves the ease of calculating distances, especially for clustering\n",
    "\n",
    "from datashader.utils import lnglat_to_meters\n",
    "\n",
    "parsed_df.loc[:, 'x'], parsed_df.loc[:, 'y'] = lnglat_to_meters(parsed_df.lon, parsed_df.lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate bearing based on next position\n",
    "\n",
    "shifted = parsed_df[['track_id', 'x', 'y']].\\\n",
    "    groupby(\"track_id\").\\\n",
    "    shift(-1).\\\n",
    "    rename(columns=lambda x: x+\"_lag\")\n",
    "parsed_df = parsed_df.join(shifted)\n",
    "\n",
    "# https://stackoverflow.com/questions/5058617/bearing-between-two-points\n",
    "\n",
    "\n",
    "def gb(x1, x2, y1, y2):\n",
    "    angle = np.arctan2(y1 - y2, x1 - x2) * 180 / np.pi\n",
    "    #     bearing1 = (angle + 360) % 360\n",
    "    bearing2 = (90 - angle) % 360\n",
    "\n",
    "    return(bearing2)\n",
    "\n",
    "\n",
    "parsed_df['bearing'] = gb(\n",
    "    x1=parsed_df['x'],\n",
    "    x2=parsed_df['x_lag'],\n",
    "    y1=parsed_df['y'],\n",
    "    y2=parsed_df['y_lag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute bearing of first points\n",
    "\n",
    "parsed_df = parsed_df.sort_values(\n",
    "    by='record_id', axis=0)  # make sure record is in order\n",
    "\n",
    "shifted = parsed_df[['track_id', 'bearing']].\\\n",
    "    groupby(\"track_id\").\\\n",
    "    shift(1).\\\n",
    "    rename(columns=lambda x: x+\"_lead\")\n",
    "parsed_df = parsed_df.join(shifted)\n",
    "\n",
    "# if bearing is null, take the previous bearing for the track id\n",
    "parsed_df['bearing'] = np.where(parsed_df['bearing'].isnull(),\n",
    "                                parsed_df['bearing_lead'], parsed_df['bearing'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there should be no more null bearing\n",
    "\n",
    "parsed_df[parsed_df['bearing'].isnull()]#[['record_id','count']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qygKrwPHQBzS"
   },
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IPtaYCiRVP2"
   },
   "outputs": [],
   "source": [
    "parsed_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o7UBe3uFS1YE"
   },
   "outputs": [],
   "source": [
    "len(parsed_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9InUmuFrEkP9"
   },
   "source": [
    "## Variable Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxX1CV8KSZ93"
   },
   "outputs": [],
   "source": [
    "# speed vs time - 25 vehicles\n",
    "\n",
    "dims = (10, 6)\n",
    "fig, ax = plt.subplots(figsize=dims)\n",
    "df=parsed_df[(parsed_df['track_id']>100) & (parsed_df['track_id']<105)]\\\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    x=\"time\",\n",
    "    y=\"speed\",\n",
    "#     hue=\"track_id\",\n",
    "    marker='x',\n",
    "    s=0.2,\n",
    "    data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Pbk-NMv3aOFv"
   },
   "outputs": [],
   "source": [
    "# lat lon - 25 vehicles\n",
    "\n",
    "dims = (10, 6)\n",
    "fig, ax = plt.subplots(figsize=dims)\n",
    "df = parsed_df[(parsed_df['track_id']>100) & (parsed_df['track_id']<125)]\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    x=\"lon\",\n",
    "    y=\"lat\",\n",
    "#     hue=\"track_id\",\n",
    "    marker='+',\n",
    "    s=1,\n",
    "    data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gK8eQGHS-SYN"
   },
   "outputs": [],
   "source": [
    "# lat lon - all vehicles\n",
    "\n",
    "dims = (10, 6)\n",
    "fig, ax = plt.subplots(figsize=dims)\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    x=\"lon\",\n",
    "    y=\"lat\",\n",
    "    #hue=\"track_id\",\n",
    "    marker='x',\n",
    "    s=0.2,\n",
    "    data=parsed_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rcce5uZ_b34Y"
   },
   "outputs": [],
   "source": [
    "# lat lon - stopped only - speed <1\n",
    "\n",
    "dims = (10, 6)\n",
    "fig, ax = plt.subplots(figsize=dims)\n",
    "df = parsed_df[parsed_df['speed']<1]\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    x=\"lon\",\n",
    "    y=\"lat\",\n",
    "    #hue=\"track_id\",\n",
    "    marker='x',\n",
    "    s=0.5,\n",
    "    data=df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBCOWwOfiTZB"
   },
   "outputs": [],
   "source": [
    "# lat lon - at a certain time frame with low speed\n",
    "\n",
    "dims = (10, 6)\n",
    "fig, ax = plt.subplots(figsize=dims)\n",
    "df = parsed_df[(parsed_df['time'] == 0) & (parsed_df['speed'] < 1)]\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    x=\"lon\",\n",
    "    y=\"lat\",\n",
    "    #     hue=\"type\",\n",
    "    # style=\"speed\",\n",
    "    marker='x',\n",
    "    s=20,\n",
    "    data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCeVLl3HjQY2"
   },
   "source": [
    "## Datashader visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0WXPnKqepuXL"
   },
   "outputs": [],
   "source": [
    "# https://datashader.org/user_guide/Geography.html\n",
    "# https://holoviews.org/reference/elements/bokeh/Tiles.html\n",
    "## hv.element.tiles.tile_sources\n",
    "\n",
    "from holoviews.element import tiles\n",
    "from datashader.utils import lnglat_to_meters\n",
    "\n",
    "df = parsed_df.copy()\n",
    "df['track_id'] = df['track_id']\n",
    "df['type'] = df['type']\n",
    "df.loc[:, 'x'], df.loc[:, 'y'] = lnglat_to_meters(df.lon,df.lat)\n",
    "df = df[['x', 'y', 'lon', 'lat', 'track_id', 'time', 'type']]\n",
    "points = hv.Points(df.copy())\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv.output(backend='bokeh')\n",
    "#tiles.EsriImagery() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrainRetina() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrain() * \n",
    "tiles.CartoLight() * hd.datashade(points).opts(hv.opts(width=750, height=350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a2nRqNHcpucx"
   },
   "outputs": [],
   "source": [
    "# https://datashader.org/user_guide/Geography.html\n",
    "# https://holoviews.org/reference/elements/bokeh/Tiles.html\n",
    "## hv.element.tiles.tile_sources\n",
    "# https://datashader.org/getting_started/Interactivity.html\n",
    "\n",
    "from holoviews.element import tiles\n",
    "from datashader.utils import lnglat_to_meters\n",
    "from datashader.colors import Sets1to3\n",
    "\n",
    "df = parsed_df.copy()\n",
    "df['track_id'] = df['track_id']\n",
    "df['type'] = df['type']\n",
    "df.loc[:, 'x'], df.loc[:, 'y'] = lnglat_to_meters(df.lon,df.lat)\n",
    "df = df[['x', 'y', 'lon', 'lat', 'track_id', 'time', 'type']]\n",
    "points = hv.Points(df.copy())\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv.output(backend='bokeh')\n",
    "plot = hd.datashade(points, aggregator=ds.count_cat('type')).opts(hv.opts(width=750, height=350))\n",
    "#tiles.EsriImagery() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrainRetina() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "\n",
    "color_key = [(name,color) for name,color in zip(['Car', 'Medium Vehicle', 'Motorcycle', 'Heavy Vehicle', 'Bus',\n",
    "       'Taxi'], Sets1to3)]\n",
    "color_points = hv.NdOverlay({n: hv.Points(df.iloc[0:1,:], label=str(n)).opts(style=dict(color=c)) for n,c in color_key})\n",
    "\n",
    "#tiles.StamenTerrain() * \n",
    "tiles.CartoLight() * plot * color_points \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6usnnhENxVcj"
   },
   "outputs": [],
   "source": [
    "# Car only\n",
    "\n",
    "# https://datashader.org/user_guide/Geography.html\n",
    "# https://holoviews.org/reference/elements/bokeh/Tiles.html\n",
    "## hv.element.tiles.tile_sources\n",
    "\n",
    "from holoviews.element import tiles\n",
    "from datashader.utils import lnglat_to_meters\n",
    "\n",
    "df = parsed_df[parsed_df['type']=='Car'].copy()\n",
    "df['track_id'] = df['track_id']\n",
    "df['type'] = df['type']\n",
    "df.loc[:, 'x'], df.loc[:, 'y'] = lnglat_to_meters(df.lon,df.lat)\n",
    "df = df[['x', 'y', 'lon', 'lat', 'track_id', 'time', 'type']]\n",
    "points = hv.Points(df.copy())\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv.output(backend='bokeh')\n",
    "#tiles.EsriImagery() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrainRetina() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrain() * \n",
    "tiles.CartoLight() * hd.datashade(points).opts(hv.opts(width=750, height=350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BCEakAmByIPs"
   },
   "outputs": [],
   "source": [
    "# Buses only\n",
    "\n",
    "# https://datashader.org/user_guide/Geography.html\n",
    "# https://holoviews.org/reference/elements/bokeh/Tiles.html\n",
    "## hv.element.tiles.tile_sources\n",
    "\n",
    "from holoviews.element import tiles\n",
    "from datashader.utils import lnglat_to_meters\n",
    "\n",
    "df = parsed_df[parsed_df['type']=='Car'].copy()\n",
    "df['track_id'] = df['track_id']\n",
    "df['type'] = df['type']\n",
    "df.loc[:, 'x'], df.loc[:, 'y'] = lnglat_to_meters(df.lon,df.lat)\n",
    "df = df[['x', 'y', 'lon', 'lat', 'track_id', 'time', 'type']]\n",
    "points = hv.Points(df.copy())\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv.output(backend='bokeh')\n",
    "#tiles.EsriImagery() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrainRetina() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrain() * \n",
    "tiles.CartoLight() * hd.datashade(points).opts(hv.opts(width=750, height=350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhhnH3951s_O"
   },
   "outputs": [],
   "source": [
    "# ~ Stationary points only\n",
    "\n",
    "# https://datashader.org/user_guide/Geography.html\n",
    "# https://holoviews.org/reference/elements/bokeh/Tiles.html\n",
    "## hv.element.tiles.tile_sources\n",
    "\n",
    "from holoviews.element import tiles\n",
    "from datashader.utils import lnglat_to_meters\n",
    "\n",
    "df = parsed_df[(parsed_df['speed']==0)].copy()\n",
    "df['track_id'] = df['track_id']\n",
    "df['type'] = df['type']\n",
    "df.loc[:, 'x'], df.loc[:, 'y'] = lnglat_to_meters(df.lon,df.lat)\n",
    "df = df[['x', 'y', 'lon', 'lat', 'track_id', 'time', 'type']]\n",
    "points = hv.Points(df.copy())\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv.output(backend='bokeh')\n",
    "#tiles.EsriImagery() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrainRetina() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrain() * \n",
    "tiles.CartoLight() * hd.datashade(points).opts(hv.opts(width=750, height=350))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UJQcz00f3J7c"
   },
   "outputs": [],
   "source": [
    "# ~ moving points only (>0)\n",
    "\n",
    "# https://datashader.org/user_guide/Geography.html\n",
    "# https://holoviews.org/reference/elements/bokeh/Tiles.html\n",
    "## hv.element.tiles.tile_sources\n",
    "\n",
    "from holoviews.element import tiles\n",
    "from datashader.utils import lnglat_to_meters\n",
    "\n",
    "df = parsed_df[(parsed_df['speed']>0)].copy()\n",
    "df['track_id'] = df['track_id']\n",
    "df['type'] = df['type']\n",
    "df.loc[:, 'x'], df.loc[:, 'y'] = lnglat_to_meters(df.lon,df.lat)\n",
    "df = df[['x', 'y', 'lon', 'lat', 'track_id', 'time', 'type']]\n",
    "points = hv.Points(df.copy())\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv.output(backend='bokeh')\n",
    "#tiles.EsriImagery() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrainRetina() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrain() * \n",
    "tiles.CartoLight() * hd.datashade(points).opts(hv.opts(width=750, height=350))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3EiLg7KvEVZK"
   },
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 580,
     "status": "ok",
     "timestamp": 1599186675429,
     "user": {
      "displayName": "Bo Wen",
      "photoUrl": "",
      "userId": "04736056844233231284"
     },
     "user_tz": 420
    },
    "id": "Dmfyj5wUJ04n",
    "outputId": "31dfde27-97c8-4115-e8dc-4048b9330576"
   },
   "outputs": [],
   "source": [
    "parsed_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 317
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1902,
     "status": "ok",
     "timestamp": 1599186676963,
     "user": {
      "displayName": "Bo Wen",
      "photoUrl": "",
      "userId": "04736056844233231284"
     },
     "user_tz": 420
    },
    "id": "4CSh3M917shE",
    "outputId": "1e5e4f56-27a8-4020-d6ba-d03828b2ab05"
   },
   "outputs": [],
   "source": [
    "parsed_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility - hdbscan clustering\n",
    "\n",
    "# https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html\n",
    "\n",
    "import hdbscan\n",
    "\n",
    "\n",
    "def cluster_hdbscan(df,\n",
    "                    parameters=None,\n",
    "                    feature_names=['x', 'y'],\n",
    "                    label_name='unnamed_cluster',\n",
    "                    verbose=True):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    default_parameters = {\n",
    "        'metric': 'euclidean',\n",
    "        'min_cluster_size': 200,\n",
    "        'min_samples': None,\n",
    "        'cluster_selection_epsilon': 7\n",
    "    }\n",
    "\n",
    "    if(parameters == None):\n",
    "        parameters = default_parameters\n",
    "    else:\n",
    "        default_parameter_names = list(default_parameters.keys())\n",
    "        parameter_names = list(parameters.keys())\n",
    "\n",
    "        for parameter in default_parameter_names:\n",
    "            if(parameter not in parameter_names):\n",
    "                parameters[parameter] = default_parameters[parameter]\n",
    "\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        metric=parameters['metric'],\n",
    "        min_cluster_size=parameters['min_cluster_size'],\n",
    "        min_samples=parameters['min_samples'],\n",
    "        cluster_selection_epsilon=parameters['cluster_selection_epsilon']\n",
    "    )\n",
    "\n",
    "    clusterer.fit(df[feature_names])\n",
    "\n",
    "    df[label_name] = clusterer.labels_\n",
    "\n",
    "    if verbose:\n",
    "        print('hdbscan trained on: ' + str(parameters))\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility - dbscan clustering\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html\n",
    "\n",
    "\n",
    "def cluster_dbscan(df,\n",
    "                   parameters=None,\n",
    "                   feature_names=['x', 'y'],\n",
    "                   label_name='unnamed_cluster',\n",
    "                   verbose=True):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "#     default_parameters = {\n",
    "#         'metric': 'euclidean',\n",
    "#         'min_cluster_size': 200,\n",
    "#         'min_samples': None,\n",
    "#         'cluster_selection_epsilon': 7\n",
    "#     }\n",
    "    clusterer = DBSCAN(\n",
    "        eps=parameters['cluster_selection_epsilon'],\n",
    "        min_samples=parameters['min_samples'],\n",
    "    ).fit(df[feature_names])\n",
    "\n",
    "    df[label_name] = clusterer.labels_\n",
    "\n",
    "    if verbose:\n",
    "        print('dbscan trained on: ' + str(parameters))\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility - kmeans clustering\n",
    "\n",
    "# https://hdbscan.readthedocs.io/en/latest/how_hdbscan_works.html\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "\n",
    "def cluster_kmeans(df,\n",
    "                   n_clusters=4,\n",
    "                   feature_names=['bearing_median'],\n",
    "                   label_name='unnamed_cluster',\n",
    "                   verbose=True):\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=0).fit(\n",
    "        df[feature_names])\n",
    "    df[label_name] = kmeans.labels_\n",
    "\n",
    "    if verbose:\n",
    "        print('kmeans trained on: ' + str(n_clusters) +\n",
    "              \" clusters and \" + str(feature_names))\n",
    "\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Road Segment Clustering\n",
    "\n",
    "Clustering roadway segments to identify apporach and major road / intersection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare segment clustering training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3071,
     "status": "ok",
     "timestamp": 1599193757126,
     "user": {
      "displayName": "Bo Wen",
      "photoUrl": "",
      "userId": "04736056844233231284"
     },
     "user_tz": 420
    },
    "id": "JivI0SwDysxt",
    "outputId": "252aee96-ad66-4925-e7bf-2f2aa8ff39ac"
   },
   "outputs": [],
   "source": [
    "# prep training data\n",
    "\n",
    "df = parsed_df  # [(parsed_df['speed']<5)].copy() # ~ bottom 75% speeds\n",
    "df['record_id'] = df['record_id']\n",
    "#df['type'] = df['type']\n",
    "seg_all_df = df[['x', 'y', 'bearing',\n",
    "                 'record_id']].set_index('record_id')\n",
    "#seg_all_df = seg_all_df.head(100000)\n",
    "\n",
    "# rounding is not a good idea\n",
    "#seg_all_df['x'] = seg_all_df['x'].round(1)\n",
    "#seg_all_df['y'] = seg_all_df['y'].round(1)\n",
    "\n",
    "# set count\n",
    "seg_all_df['count'] = 1\n",
    "\n",
    "# get count and angle by unique location\n",
    "seg_all_df = seg_all_df.\\\n",
    "    groupby(['x', 'y']).\\\n",
    "    agg({\"count\": np.sum, 'bearing': np.median}).\\\n",
    "    reset_index()\n",
    "\n",
    "# get total and pct of count\n",
    "seg_all_df['total_count'] = seg_all_df['count'].sum()\n",
    "seg_all_df['count_pct'] = seg_all_df['count'] / \\\n",
    "    seg_all_df['total_count'] * 100\n",
    "\n",
    "# save all data for unique points\n",
    "seg_all_df = seg_all_df.reset_index(\n",
    "    drop=False).rename(columns={'index': 'u_id'}).set_index('u_id')\n",
    "\n",
    "### DENSITY REDUCTION ###\n",
    "# # filter out unique points with fewer than 0.05% of total points\n",
    "# seg_all_df = seg_all_df[seg_all_df['count_pct'] > 0.05]\n",
    "\n",
    "# # filter out unique points with fewer than 0.0001% of total points (1 in mil)\n",
    "# seg_all_df = seg_all_df[seg_all_df['count_pct']>0.0002]\n",
    "\n",
    "# filter out infreq points (points with less than 10 samples) for training\n",
    "# this helps reduce data size and introduce breaks in low density areas of the data\n",
    "seg_train_df = seg_all_df[seg_all_df['count'] > 10]\n",
    "seg_infre_df = seg_all_df[seg_all_df['count'] <= 10]\n",
    "\n",
    "# choose features to be trained on - not needed!\n",
    "# seg_train_df = seg_train_df[['x', 'y', 'count', 'count_pct']]\n",
    "# seg_train_df = seg_train_df[['x', 'y', 'bearing']]\n",
    "# seg_train_df = seg_train_df[['x', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# full dataset of unique points (all points)\n",
    "seg_all_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training dataset of unique points (only frequent points)\n",
    "seg_train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# infrequent data points excluded from training\n",
    "seg_infre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fzOQ2ta0mpQ0"
   },
   "outputs": [],
   "source": [
    "# visual inspect - lat lon\n",
    "\n",
    "dims = (10, 6)\n",
    "fig, ax = plt.subplots(figsize=dims)\n",
    "\n",
    "ax = sns.scatterplot(\n",
    "    x='x',\n",
    "    y='y',\n",
    "    s=1,\n",
    "    palette=\"black\",\n",
    "    # hue=\"count\",\n",
    "    # style=\"speed\",\n",
    "    marker='+',\n",
    "    edgecolors='red',\n",
    "    data=seg_train_df.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 367,
     "output_embedded_package_id": "1CfIoIrE7a-eEL-YgxZ5JfeWdoAsOmsrk"
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2243,
     "status": "ok",
     "timestamp": 1599189327061,
     "user": {
      "displayName": "Bo Wen",
      "photoUrl": "",
      "userId": "04736056844233231284"
     },
     "user_tz": 420
    },
    "id": "rag4MMBoDObW",
    "outputId": "9316aece-003d-4d33-d92c-e52e1d6e2537"
   },
   "outputs": [],
   "source": [
    "# visual inspect - rasterize lat lon\n",
    "\n",
    "# https://datashader.org/user_guide/Geography.html\n",
    "# https://holoviews.org/reference/elements/bokeh/Tiles.html\n",
    "## hv.element.tiles.tile_sources\n",
    "\n",
    "from holoviews.element import tiles\n",
    "from datashader.utils import lnglat_to_meters\n",
    "# https://datashader.org/getting_started/Interactivity.html\n",
    "from datashader.colors import Sets1to3\n",
    "# https://github.com/holoviz/datashader/issues/767\n",
    "import colorcet as cc\n",
    "long_key = list(set(cc.glasbey_cool + cc.glasbey_warm + cc.glasbey_dark))\n",
    "\n",
    "df = seg_train_df.copy()\n",
    "#df['seg_cluster'] = df['seg_cluster'].apply(lambda x: 0 if x >=0 else -1)\n",
    "points = hv.Points(df.copy())\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv.output(backend='bokeh')\n",
    "#tiles.EsriImagery() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrainRetina() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrain() * \n",
    "tiles.CartoLight() * hd.datashade(points, \n",
    "             #aggregator=ds.count_cat('seg_cluster'), \n",
    "             #color_key=long_key\n",
    "             ).opts(hv.opts(width=750, height=350))\n",
    "#hd.dynspread(hd.datashade(points, \n",
    "#             aggregator=ds.count_cat('seg_cluster'), d\n",
    "#             color_key=Sets1to3).opts(hv.opts(width=750, height=350)), threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HDBSCAN for Roadway Segment Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define subclustering parameters\n",
    "\n",
    "seg_cluster_parameter = {    \n",
    "    \n",
    "    # x clusters of medium size segments\n",
    "    'metric': 'euclidean',\n",
    "    'min_cluster_size': 150,\n",
    "    'min_samples': None,\n",
    "    'cluster_selection_epsilon': 5\n",
    "    \n",
    "#     # 8 clusters of medium size segments\n",
    "#     'metric': 'euclidean',\n",
    "#     'min_cluster_size': 200,\n",
    "#     'min_samples': None,\n",
    "#     'cluster_selection_epsilon': 20\n",
    "\n",
    "    # # 7 clusters of medium size segments\n",
    "    #     'metric'='euclidean',\n",
    "    #     'min_cluster_size'=300,\n",
    "    #     'min_samples'=None,\n",
    "    #     'cluster_selection_epsilon'=10\n",
    "\n",
    "    # # 12 clusters of fine segments\n",
    "    #     'metric'='euclidean',\n",
    "    #     'min_cluster_size'=150,\n",
    "    #     'min_samples'=None,\n",
    "    #     'cluster_selection_epsilon'=5\n",
    "}\n",
    "\n",
    "# run subclustering for lanes\n",
    "seg_train_df_1 = cluster_hdbscan(df=seg_train_df,\n",
    "                               parameters=seg_cluster_parameter,\n",
    "                               feature_names=['x', 'y'],\n",
    "                               label_name='seg_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seg_train_df_1['seg_cluster'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual inspect clusters by facet plot\n",
    "\n",
    "# https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\n",
    "\n",
    "g = sns.FacetGrid(seg_train_df_1, col='seg_cluster', col_wrap=5, height=4)\n",
    "g = g.map(plt.scatter, 'x', 'y', s=0.1)#, edgecolor=\"w\")\n",
    "\n",
    "# note: cluster 3 and 8&9 are of interest, manually merge 8 and 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### HDBSCAN parameter tuning decisions ###\n",
    "# https://hdbscan.readthedocs.io/en/latest/parameter_selection.html\n",
    "# min_samples\n",
    "# opt A for decision for min_sample for core points\n",
    "# ~ 400*600m^2 (240,000 m^2 area)\n",
    "# ~ 1 mil unique points (745,709 if no lone points)\n",
    "# avg density of points or minimum eligible density should be ~ 5 points\n",
    "# opt B for decision for min_sample for core points\n",
    "# net area is ~ (based on rough calculation of roadway areas)\n",
    "# 50*600 + 30*400 + 4 * 10*400 = 58,000\n",
    "# ~ 1 mil unique points (745,709 if no lone points)\n",
    "# avg density of points or minimum eligible density should be ~ 15 points\n",
    "# option A or B generates way too many clusters, gradully increase min_samples for core points until less clusters are generated\n",
    "\n",
    "# cluster_selection_epsilon\n",
    "# 1.5m radius (or 3.0m width) is approx. lane width, use 5m for a typ. 3 lane roadway\n",
    "\n",
    "# HDBSCAN(algorithm='best', alpha=1.0, approx_min_span_tree=True,\n",
    "#    gen_min_span_tree=False, leaf_size=40, memory=Memory(cachedir=None),\n",
    "#    metric='euclidean', min_cluster_size=5, min_samples=None, p=None)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for second-stage training\n",
    "seg_train_df_2 = seg_train_df_1[seg_train_df_1['seg_cluster']==-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_train_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd stage dbscan clustering\n",
    "# with more relax parameters on unclustered point from 1st stage only\n",
    "\n",
    "# define subclustering parameters\n",
    "\n",
    "seg_cluster_parameter = {\n",
    "    \n",
    "    # x clusters of medium size segments\n",
    "    'metric': 'euclidean',\n",
    "    'min_cluster_size': 75,\n",
    "    'min_samples': None,\n",
    "    'cluster_selection_epsilon': 5\n",
    "}\n",
    "\n",
    "# run subclustering for lanes\n",
    "seg_train_df_2 = cluster_hdbscan(df=seg_train_df_2,\n",
    "                               parameters=seg_cluster_parameter,\n",
    "                               feature_names=['x', 'y'],\n",
    "                               label_name='seg_cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_train_df_2[seg_train_df_2['seg_cluster']==-1]\n",
    "\n",
    "# clustered from stage 1\n",
    "seg_a = seg_train_df_1[seg_train_df_1['seg_cluster'] != -1].copy()\n",
    "\n",
    "# clustered from stage 2\n",
    "seg_b = seg_train_df_2[seg_train_df_2['seg_cluster'] != -1].copy()\n",
    "prev_max = seg_train_df_1[seg_train_df_1['seg_cluster']\n",
    "                          != -1]['seg_cluster'].max()\n",
    "seg_b['seg_cluster'] = seg_b['seg_cluster'] + \\\n",
    "    prev_max + 1  # increment cluster number\n",
    "\n",
    "# unclustered\n",
    "seg_c = seg_train_df_2[seg_train_df_2['seg_cluster'] == -1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# update training data\n",
    "seg_train_df = pd.concat([seg_a,seg_b,seg_c])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual inspect clusters by facet plot\n",
    "\n",
    "# https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\n",
    "\n",
    "g = sns.FacetGrid(seg_train_df_2, col='seg_cluster', col_wrap=5, height=4)\n",
    "g = g.map(plt.scatter, 'x', 'y', s=0.1)#, edgecolor=\"w\")\n",
    "\n",
    "# cluster 6+12 = 18 is of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual inspect clusters by facet plot\n",
    "\n",
    "# https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\n",
    "\n",
    "g = sns.FacetGrid(seg_train_df, col='seg_cluster', col_wrap=5, height=4)\n",
    "g = g.map(plt.scatter, 'x', 'y', s=0.1)#, edgecolor=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting only clusters of interests:\n",
    "# -Cluster 8 + 9 (E-W road),\n",
    "# -Cluster 3 (N-S road in the NW corner),\n",
    "# -Cluster 18 (turning lane from South road to E-W road)\n",
    "\n",
    "seg_train_df['seg_cluster_combined'] = seg_train_df['seg_cluster'].\\\n",
    "    apply(lambda x: 'A' if ((x == 'A') | (x == 8) | (x == 9))\n",
    "          else ('B' if ((x == 'B') | (x == 3)) else\n",
    "                'C' if ((x == 'C') | (x == 18))\n",
    "                else 'Exclude'\n",
    "                )\n",
    "          )\n",
    "\n",
    "\n",
    "# remove cluster to be excluded, assign combined cluster as final cluster\n",
    "\n",
    "seg_train_df = seg_train_df[seg_train_df['seg_cluster_combined'].\n",
    "                            isin(['A', 'B', 'C'])]\n",
    "seg_train_df['seg_cluster'] = seg_train_df['seg_cluster_combined']\n",
    "seg_train_df = seg_train_df.drop(columns=['seg_cluster_combined'])\n",
    "seg_train_df.groupby(['seg_cluster']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 360484,
     "status": "aborted",
     "timestamp": 1599190403293,
     "user": {
      "displayName": "Bo Wen",
      "photoUrl": "",
      "userId": "04736056844233231284"
     },
     "user_tz": 420
    },
    "id": "8Ko6WH5AEuTQ"
   },
   "outputs": [],
   "source": [
    "# visual inspect clusters by map - color by clusters\n",
    "\n",
    "# https://datashader.org/user_guide/Geography.html\n",
    "# https://holoviews.org/reference/elements/bokeh/Tiles.html\n",
    "## hv.element.tiles.tile_sources\n",
    "\n",
    "from holoviews.element import tiles\n",
    "from datashader.utils import lnglat_to_meters\n",
    "# https://datashader.org/getting_started/Interactivity.html\n",
    "from datashader.colors import Sets1to3\n",
    "# https://github.com/holoviz/datashader/issues/767\n",
    "import colorcet as cc\n",
    "long_key = list(set(cc.glasbey_cool + cc.glasbey_warm + cc.glasbey_dark))\n",
    "\n",
    "df = seg_train_df#[seg_train_df['seg_cluster']>=0].copy()\n",
    "# df = seg_train_df[seg_train_df['seg_cluster']==0].copy()\n",
    "points = hv.Points(df.copy())\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv.output(backend='bokeh')\n",
    "#tiles.EsriImagery() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrainRetina() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrain() * \n",
    "tiles.CartoLight() * hd.datashade(points, \n",
    "             aggregator=ds.count_cat('seg_cluster'), \n",
    "             color_key=Sets1to3).opts(hv.opts(width=750, height=350))\n",
    "#tiles.CartoLight() * hd.dynspread(hd.datashade(points, \n",
    "#             aggregator=ds.count_cat('seg_cluster'), \n",
    "#             color_key=long_key).opts(hv.opts(width=750, height=350)), threshold=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-processing for un-clustered points\n",
    "\n",
    "recover some nearby points not classified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### un-clustered points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reassignment part A for unclustered points\n",
    "\n",
    "# approach #1\n",
    "# - every point within an existing cluster is used as a core point for cluster reassignment\n",
    "# - this approach require a lot more distance computations\n",
    "\n",
    "seg_train_df_0 = seg_train_df[seg_train_df['seg_cluster'] == -1].\\\n",
    "    reset_index(drop=False).\\\n",
    "    rename(columns={'u_id': 'u_id'})\n",
    "seg_train_df_1 = seg_train_df[seg_train_df['seg_cluster'] != -1].\\\n",
    "    reset_index(drop=False).\\\n",
    "    rename(columns={'u_id': 'u_id_clustered'})\n",
    "\n",
    "seg_train_df_0 = seg_train_df_0.drop(columns=['seg_cluster'])\n",
    "seg_train_df_1 = seg_train_df_1.\\\n",
    "    rename(columns={'x': 'x_clustered', 'y': 'y_clustered'}).\\\n",
    "    drop(columns=['count', 'bearing', 'total_count', 'count_pct'])\n",
    "\n",
    "seg_train_df_0['tmp'] = 1\n",
    "seg_train_df_1['tmp'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seg_train_df_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seg_train_df_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build intermediate dataframe\n",
    "# https://stackoverflow.com/questions/35234012/python-pandas-merge-two-tables-without-keys-multiply-2-dataframes-with-broadc\n",
    "seg_train_df_reassign_a = pd.merge(seg_train_df_0, seg_train_df_1, on=['tmp']).drop(columns='tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Euclidean distance\n",
    "# more resources for more complex examples: https://kanoki.org/2019/12/27/how-to-calculate-distance-in-python-and-pandas-using-scipy-spatial-and-distance-functions/\n",
    "def e_dist(x1, x2, y1, y2):\n",
    "    return np.sqrt((x1-x2) ** 2+(y1-y2) ** 2)\n",
    "\n",
    "\n",
    "df = seg_train_df_reassign_a\n",
    "\n",
    "df['dist'] = e_dist(\n",
    "    x1=df['x_clustered'],\n",
    "    x2=df['x'],\n",
    "    y1=df['y_clustered'],\n",
    "    y2=df['y'])\n",
    "\n",
    "# get minimum distance in each group\n",
    "idx = df.groupby(['u_id'])['dist'].transform(min) == df['dist']\n",
    "\n",
    "# save results\n",
    "seg_reassigned_df_a = df.copy()\n",
    "seg_reassigned_idx_a = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit on reassigning unclustered points\n",
    "reassign_dist_limit = 20 # meters\n",
    "\n",
    "seg_unclustered_df = seg_reassigned_df_a[seg_reassigned_idx_a]\n",
    "# limit max distance to 20 meters\n",
    "seg_unclustered_df = seg_unclustered_df[seg_unclustered_df['dist'] < reassign_dist_limit]\n",
    "seg_unclustered_df = seg_unclustered_df.set_index('u_id')\n",
    "seg_unclustered_df = seg_unclustered_df[list(seg_train_df.columns)]\n",
    "seg_unclustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(seg_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_train_df_final = pd.concat(\n",
    "    [seg_train_df[seg_train_df['seg_cluster'] != -1], seg_unclustered_df])\n",
    "\n",
    "seg_train_df_final"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A quick look at convex hull with the clustered and unclusted points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build convex hull to recapture raw gps points\n",
    "# https://stackoverflow.com/questions/60194404/how-to-make-a-polygon-shapefile-which-corresponds-to-the-outer-boundary-of-the-g\n",
    "# https://docs.scipy.org/doc/scipy/reference/generated/scipy.spatial.ConvexHull.html\n",
    "\n",
    "# # scipy convex hull example\n",
    "\n",
    "# from scipy.spatial import ConvexHull, convex_hull_plot_2d\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # hull 1\n",
    "# points = np.random.rand(30, 2)   # 30 random points in 2-D\n",
    "# hull = ConvexHull(points)\n",
    "# plt.plot(points[:,0], points[:,1], 'o')\n",
    "# for simplex in hull.simplices:\n",
    "#     plt.plot(points[simplex, 0], points[simplex, 1], 'k-')\n",
    "    \n",
    "# # hull 2\n",
    "# points = np.random.rand(30, 2)   # 30 random points in 2-D\n",
    "# plt.plot(points[:,0], points[:,1], 'o')\n",
    "# hull = ConvexHull(points)\n",
    "# for simplex in hull.simplices:\n",
    "#     plt.plot(points[simplex, 0], points[simplex, 1], 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at convex hull without unclustered points\n",
    "# this can be thought of as congested areas\n",
    "\n",
    "df = seg_train_df[seg_train_df['seg_cluster'] != -1].\\\n",
    "    reset_index(drop=False).\\\n",
    "    rename(columns={'u_id': 'u_id_clustered'})\n",
    "\n",
    "# build an dictionary of convex hull points\n",
    "cluster_pt_dict = {}\n",
    "for cluster in df['seg_cluster'].unique():\n",
    "    cluster_pt_dict[cluster] = df[\n",
    "        df['seg_cluster'] == cluster][['x', 'y']].to_numpy()\n",
    "\n",
    "\n",
    "def get_convex_hull_indices(pts_array):\n",
    "    hull = ConvexHull(pts_array)\n",
    "    hull_indices = np.unique(hull.simplices.flat)\n",
    "    hull_pts = pts_array[hull_indices, :]\n",
    "    return(hull_pts)\n",
    "\n",
    "\n",
    "# get convex hull\n",
    "cluster_hull_dict = {}\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    cluster_hull_dict[cluster] = get_convex_hull_indices(\n",
    "        cluster_pt_dict[cluster])\n",
    "\n",
    "\n",
    "# plot\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    plt.plot(cluster_pt_dict[cluster][:, 0],\n",
    "             cluster_pt_dict[cluster][:, 1], ',')\n",
    "    hull = ConvexHull(cluster_pt_dict[cluster])\n",
    "    for simplex in hull.simplices:\n",
    "        plt.plot(cluster_pt_dict[cluster][simplex, 0],\n",
    "                 cluster_pt_dict[cluster][simplex, 1], 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at convex hull with unclustered points\n",
    "# this can be viewed as extended congested areas\n",
    "\n",
    "df = seg_train_df_final.copy()\n",
    "\n",
    "# build an dictionary of convex hull points\n",
    "cluster_pt_dict = {}\n",
    "for cluster in df['seg_cluster'].unique():\n",
    "    cluster_pt_dict[cluster] = df[\n",
    "        df['seg_cluster'] == cluster][['x', 'y']].to_numpy()\n",
    "\n",
    "\n",
    "def get_convex_hull_indices(pts_array):\n",
    "    hull = ConvexHull(pts_array)\n",
    "    hull_indices = np.unique(hull.simplices.flat)\n",
    "    hull_pts = pts_array[hull_indices, :]\n",
    "    return(hull_pts)\n",
    "\n",
    "\n",
    "# get convex hull objects\n",
    "cluster_hull_objs = {}\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    cluster_hull_objs[cluster] = ConvexHull(cluster_pt_dict[cluster])\n",
    "\n",
    "# get convex hull indice points\n",
    "cluster_hull_dict = {}\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    cluster_hull_dict[cluster] = get_convex_hull_indices(\n",
    "        cluster_pt_dict[cluster])\n",
    "\n",
    "\n",
    "# plot\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    plt.plot(cluster_pt_dict[cluster][:, 0],\n",
    "             cluster_pt_dict[cluster][:, 1], ',')\n",
    "    hull = ConvexHull(cluster_pt_dict[cluster])\n",
    "    for simplex in hull.simplices:\n",
    "        plt.plot(cluster_pt_dict[cluster][simplex, 0],\n",
    "                 cluster_pt_dict[cluster][simplex, 1], 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# build a convex hull points df from the entire training set (incl. unclustered points)\n",
    "\n",
    "cluster_hull_list_df = []\n",
    "for cluster in list(cluster_hull_dict.keys()):\n",
    "    label = cluster\n",
    "    df = pd.DataFrame(cluster_hull_dict[cluster], columns=['x', 'y'])\n",
    "    df['seg_cluster'] = label\n",
    "    cluster_hull_list_df.append(df)\n",
    "    \n",
    "cluster_hull_df = pd.concat(cluster_hull_list_df)\n",
    "\n",
    "cluster_hull_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Road Segment to all unique data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16750618/whats-an-efficient-way-to-find-if-a-point-lies-in-the-convex-hull-of-a-point-cl/16898636#16898636\n",
    "\n",
    "def in_hull(p, hull):\n",
    "    \"\"\"\n",
    "    Test if points in `p` are in `hull`\n",
    "\n",
    "    `p` should be a `NxK` coordinates of `N` points in `K` dimensions\n",
    "    `hull` is either a scipy.spatial.Delaunay object or the `MxK` array of the \n",
    "    coordinates of `M` points in `K`dimensions for which Delaunay triangulation\n",
    "    will be computed\n",
    "    \"\"\"\n",
    "    from scipy.spatial import Delaunay\n",
    "    if not isinstance(hull, Delaunay):\n",
    "        hull = Delaunay(hull)\n",
    "\n",
    "    return hull.find_simplex(p) >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over convex hull objects and match points\n",
    "\n",
    "cluster_hull_objs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parsed_df.copy()\n",
    "\n",
    "df['x_id'] = df['x'] * 100\n",
    "df['x_id'] = df['x_id'].astype(int)\n",
    "df['y_id'] = df['y'] * 100\n",
    "df['y_id'] = df['y_id'].astype(int)\n",
    "\n",
    "# save ids to parsed_df\n",
    "parsed_df = df.copy()\n",
    "\n",
    "df['count'] = 1\n",
    "\n",
    "# get count and angle by unique location\n",
    "df = df.\\\n",
    "    groupby(['x', 'y', 'x_id', 'y_id']).\\\n",
    "    agg({\"count\": np.sum, 'bearing': np.median}).\\\n",
    "    rename(columns={'bearing': 'bearing_median'}).\\\n",
    "    reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_cols = []\n",
    "cluster_keys = list(cluster_hull_dict.keys())\n",
    "cluster_keys.sort()\n",
    "\n",
    "for cluster_hull in cluster_keys:\n",
    "    col_name = \"cluster_{}\".format(str(cluster_hull))\n",
    "    all_cluster_cols.append(col_name)\n",
    "    df[col_name] = in_hull(\n",
    "        p=df[['x', 'y']].to_numpy(),\n",
    "        hull=cluster_hull_dict[cluster_hull])\n",
    "    df.loc[df[col_name]==True, 'seg_cluster'] = str(cluster_hull)\n",
    "    df = df.drop(columns=[col_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge id table with name table\n",
    "\n",
    "# use all points - allow duplicate identicals\n",
    "# clustered_df = parsed_df.merge(\n",
    "#     df.drop(columns=['x', 'y']), on=['x_id', 'y_id'])\n",
    "\n",
    "# use only unique points - disallow identicals\n",
    "\n",
    "seg_train_df_final = df.copy()\n",
    "\n",
    "seg_train_df_final['seg_cluster'] = seg_train_df_final['seg_cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lane and Directional Sub-Clustering\n",
    "\n",
    "(Instead of Directional due to restricted scope in analysis area)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_train_df_final_bk = seg_train_df_final.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seg_train_df_final = seg_train_df_final_bk.copy()\n",
    "\n",
    "# filter out infreq points (points with less than 2 samples) for training\n",
    "# this helps reduce data size and introduce breaks in low density areas of the data\n",
    "seg_train_df_final = seg_train_df_final[seg_train_df_final['count'] > 2]\n",
    "seg_train_df_infre = seg_train_df_final[seg_train_df_final['count'] <= 2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_list = seg_train_df_final['seg_cluster'].unique()\n",
    "# cluster_list = [1]\n",
    "cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_train_df_final = seg_train_df_final[seg_train_df_final['seg_cluster']!='nan']\n",
    "\n",
    "len(seg_train_df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_list = seg_train_df_final['seg_cluster'].unique()\n",
    "# cluster_list = [1]\n",
    "cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "\n",
    "seg_train_df_final_dict = dict((key, seg_train_df_final[seg_train_df_final['seg_cluster'] == key])\n",
    "                               for key in cluster_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run subclustering for direction - kmeans\n",
    "\n",
    "\n",
    "# subcluster_parameters = {\n",
    "#     'A': {\n",
    "#         'n_clusters': 4\n",
    "#     },\n",
    "#     'B': {\n",
    "#         'n_clusters': 1\n",
    "#     },\n",
    "#     'C': {\n",
    "#         'n_clusters': 3\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "# subcluster_results = dict((key,\n",
    "#                            cluster_kmeans(df=seg_train_df_final_dict[key],\n",
    "#                                           n_clusters=subcluster_parameters[key]['n_clusters'],\n",
    "#                                           feature_names=['bearing_median'],\n",
    "#                                           label_name='dir_cluster')\n",
    "#                            )\n",
    "#                           for key in cluster_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # run subclustering for direction - hdbscan\n",
    "\n",
    "\n",
    "# # define subclustering parameters\n",
    "\n",
    "# subcluster_parameters = {\n",
    "#     'A': {\n",
    "#         'metric': 'euclidean',\n",
    "#         'min_cluster_size': 1000,\n",
    "#         'min_samples': 100,\n",
    "#         'cluster_selection_epsilon': 1\n",
    "#     },\n",
    "#     'B': {\n",
    "#         'metric': 'euclidean',\n",
    "#         'min_cluster_size': 1000,\n",
    "#         'min_samples': 100,\n",
    "#         'cluster_selection_epsilon': 1\n",
    "#     },\n",
    "#     'C': {\n",
    "#         'metric': 'euclidean',\n",
    "#         'min_cluster_size': 1000,\n",
    "#         'min_samples': 100,\n",
    "#         'cluster_selection_epsilon': 1\n",
    "#     }\n",
    "# }\n",
    "\n",
    "\n",
    "# # run subclustering for lanes\n",
    "# subcluster_results = dict((key,\n",
    "#                            cluster_hdbscan(df=seg_train_df_final_dict[key],\n",
    "#                                            parameters=subcluster_parameters[key],\n",
    "#                                            feature_names=['x', 'y'],\n",
    "#                                            label_name='dir_cluster')\n",
    "#                            )\n",
    "#                           for key in cluster_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lane_parameter = {\n",
    "    'A': {\n",
    "        'min_samples': 100,\n",
    "        'cluster_selection_epsilon': 1\n",
    "    },\n",
    "    'B': {\n",
    "        'min_samples': 100,\n",
    "        'cluster_selection_epsilon': 1\n",
    "    },\n",
    "    'C': {\n",
    "        'min_samples': 50,\n",
    "        'cluster_selection_epsilon': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "subcluster_results = dict((key, cluster_dbscan(df=seg_train_df_final_dict[key],\n",
    "                                               parameters=lane_parameter[key],\n",
    "                                               feature_names=['x', 'y'],\n",
    "                                               label_name='dir_cluster',\n",
    "                                               verbose=False)) for key in cluster_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subcluster_results_df = pd.concat(list(subcluster_results.values()))\n",
    "\n",
    "# # filter out \"outliers\" within the cluster\n",
    "# subcluster_results_df = subcluster_results_df[subcluster_results_df['lane_subcluster']!=-1]\n",
    "\n",
    "subcluster_results_df['seg_dir_cluster'] = subcluster_results_df['seg_cluster'].astype(\n",
    "    str) + \"_\" + subcluster_results_df['dir_cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subcluster_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_cluster_size = 150 # for seg dir cluster, if not met, cluster is deleted\n",
    "\n",
    "checksum = subcluster_results_df.groupby(['seg_dir_cluster']).count()\n",
    "exclude = checksum[checksum<min_cluster_size].dropna().reset_index()\n",
    "\n",
    "# exclude\n",
    "subcluster_results_df = subcluster_results_df[~subcluster_results_df['seg_dir_cluster'].isin(\n",
    "    exclude['seg_dir_cluster'])].copy()\n",
    "\n",
    "exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(subcluster_results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# taking a look at convex hull with directions\n",
    "# this can be viewed as extended congested areas\n",
    "\n",
    "df = subcluster_results_df.copy()\n",
    "\n",
    "# build an dictionary of convex hull points\n",
    "cluster_pt_dict = {}\n",
    "for cluster in df['seg_dir_cluster'].unique():\n",
    "    cluster_pt_dict[cluster] = df[\n",
    "        df['seg_dir_cluster'] == cluster][['x', 'y']].to_numpy()\n",
    "\n",
    "\n",
    "def get_convex_hull_indices(pts_array):\n",
    "    hull = ConvexHull(pts_array)\n",
    "    hull_indices = np.unique(hull.simplices.flat)\n",
    "    hull_pts = pts_array[hull_indices, :]\n",
    "    return(hull_pts)\n",
    "\n",
    "\n",
    "# get convex hull objects\n",
    "cluster_hull_objs = {}\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    cluster_hull_objs[cluster] = ConvexHull(cluster_pt_dict[cluster])\n",
    "\n",
    "# get convex hull indice points\n",
    "cluster_hull_dict = {}\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    cluster_hull_dict[cluster] = get_convex_hull_indices(\n",
    "        cluster_pt_dict[cluster])\n",
    "\n",
    "\n",
    "# plot\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    plt.plot(cluster_pt_dict[cluster][:, 0],\n",
    "             cluster_pt_dict[cluster][:, 1], ',')\n",
    "    hull = ConvexHull(cluster_pt_dict[cluster])\n",
    "    for simplex in hull.simplices:\n",
    "        plt.plot(cluster_pt_dict[cluster][simplex, 0],\n",
    "                 cluster_pt_dict[cluster][simplex, 1], 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check size of clusters\n",
    "\n",
    "subcluster_results_df.groupby('seg_dir_cluster').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points.array()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build seg_dir_lane_cluster from visual inspection\n",
    "# this effectively clean up the clustering result and get rid of clusters that aren't meaningful\n",
    "\n",
    "subcluster_results_df['seg_dir_lane_cluster'] = subcluster_results_df['seg_dir_cluster'].\\\n",
    "    apply(lambda x:\n",
    "          'Green_1' if ((x == 'B_3') | (x == 'B_4') | (x == 'B_5') | (x == 'B_6'))\n",
    "          else (\n",
    "              'Green_2' if ((x == 'B_0'))\n",
    "              else (\n",
    "                  'Green_3' if ((x == 'B_1') | (x == 'B_2'))\n",
    "                  else(\n",
    "                      'Yellow_1' if ((x == 'C_0'))\n",
    "                      else (\n",
    "                          'Yellow_2' if ((x == 'C_1'))\n",
    "                          else (\n",
    "                              'Yellow_3' if ((x == 'C_2'))\n",
    "                              else (\n",
    "                                  'Red_1' if ((x == 'A_0') | (x == 'A_3'))\n",
    "                                  else (\n",
    "                                      'Red_2' if ((x == 'A_1') | (x == 'A_7') | (x == 'A_8') | (x == 'A_15'))\n",
    "                                      else (\n",
    "                                          'Red_3' if ((x == 'A_2') | (x == 'A_9'))\n",
    "                                          else(\n",
    "                                              'Exclude'\n",
    "                                          )\n",
    "                                      )\n",
    "                                  )\n",
    "                              )\n",
    "                          )\n",
    "                      )\n",
    "                  )\n",
    "              )\n",
    "          ))\n",
    "\n",
    "subcluster_results_df = subcluster_results_df[subcluster_results_df['seg_dir_lane_cluster']!='Exclude']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visual inspect clusters by map - color by clusters\n",
    "\n",
    "# https://datashader.org/user_guide/Geography.html\n",
    "# https://holoviews.org/reference/elements/bokeh/Tiles.html\n",
    "## hv.element.tiles.tile_sources\n",
    "\n",
    "from holoviews.element import tiles\n",
    "from datashader.utils import lnglat_to_meters\n",
    "# https://datashader.org/getting_started/Interactivity.html\n",
    "from datashader.colors import Sets1to3\n",
    "# https://github.com/holoviz/datashader/issues/767\n",
    "import colorcet as cc\n",
    "long_key = list(set(cc.glasbey_cool + cc.glasbey_warm + cc.glasbey_dark))\n",
    "\n",
    "df = subcluster_results_df#[seg_train_df['seg_cluster']>=0].copy()\n",
    "# df = subcluster_results_df[subcluster_results_df['dir_cluster']>=0].copy()\n",
    "# df = subcluster_results_df[subcluster_results_df['seg_dir_cluster'].isin(['A_0', 'A_1', 'A_2', 'A_3', 'A_4', 'A_5', 'A_6', 'A_7', 'A_8', 'A_9', 'A_10', 'A_11', 'A_12', 'A_13', 'A_14', 'A_15', 'A_16', 'A_17', 'A_18'])].copy()\n",
    "# df = subcluster_results_df[subcluster_results_df['seg_dir_cluster'].isin(['A_0', 'A_3', 'A_10'])].copy()\n",
    "# df = seg_train_df[seg_train_df['seg_cluster']==0].copy()\n",
    "points = hv.Points(df.copy())\n",
    "\n",
    "hv.extension('bokeh')\n",
    "hv.output(backend='bokeh')\n",
    "#tiles.EsriImagery() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrainRetina() * hd.datashade(points).opts(hv.opts(width=750, height=350))\n",
    "#tiles.StamenTerrain() * \n",
    "\n",
    "tiles.CartoLight() * hd.datashade(points, \n",
    "             aggregator=ds.count_cat('seg_dir_lane_cluster'), \n",
    "             color_key=long_key).opts(hv.opts(width=750, height=350)) #* color_points\n",
    "#tiles.CartoLight() * hd.dynspread(hd.datashade(points, \n",
    "#             aggregator=ds.count_cat('seg_cluster'), \n",
    "#             color_key=long_key).opts(hv.opts(width=750, height=350)), threshold=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual inspect clusters by facet plot\n",
    "\n",
    "# https://seaborn.pydata.org/generated/seaborn.FacetGrid.html\n",
    "# subcluster_results_df['tmp'] = 1\n",
    "g = sns.FacetGrid(\n",
    "    subcluster_results_df,\n",
    "    hue='seg_dir_lane_cluster',\n",
    "    col_wrap=5,\n",
    "    height=4,\n",
    "    legend_out=True,\n",
    "    #     col='tmp'\n",
    "    col='seg_dir_lane_cluster'\n",
    ")\n",
    "g = g.map(plt.scatter, 'x', 'y', s=0.05, marker='.')  # , edgecolor=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Megre results with full parsed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking a look at convex hull with lane and directions\n",
    "# this can be viewed as extended congested areas\n",
    "\n",
    "df = subcluster_results_df.copy()\n",
    "\n",
    "# build an dictionary of convex hull points\n",
    "cluster_pt_dict = {}\n",
    "for cluster in df['seg_dir_lane_cluster'].unique():\n",
    "    cluster_pt_dict[cluster] = df[\n",
    "        df['seg_dir_lane_cluster'] == cluster][['x', 'y']].to_numpy()\n",
    "\n",
    "\n",
    "def get_convex_hull_indices(pts_array):\n",
    "    hull = ConvexHull(pts_array)\n",
    "    hull_indices = np.unique(hull.simplices.flat)\n",
    "    hull_pts = pts_array[hull_indices, :]\n",
    "    return(hull_pts)\n",
    "\n",
    "\n",
    "# get convex hull objects\n",
    "cluster_hull_objs = {}\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    cluster_hull_objs[cluster] = ConvexHull(cluster_pt_dict[cluster])\n",
    "\n",
    "# get convex hull indice points\n",
    "cluster_hull_dict = {}\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    cluster_hull_dict[cluster] = get_convex_hull_indices(\n",
    "        cluster_pt_dict[cluster])\n",
    "\n",
    "\n",
    "# plot\n",
    "for cluster in list(cluster_pt_dict.keys()):\n",
    "    plt.plot(cluster_pt_dict[cluster][:, 0],\n",
    "             cluster_pt_dict[cluster][:, 1], ',')\n",
    "    hull = ConvexHull(cluster_pt_dict[cluster])\n",
    "    for simplex in hull.simplices:\n",
    "        plt.plot(cluster_pt_dict[cluster][simplex, 0],\n",
    "                 cluster_pt_dict[cluster][simplex, 1], 'k-')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/16750618/whats-an-efficient-way-to-find-if-a-point-lies-in-the-convex-hull-of-a-point-cl/16898636#16898636\n",
    "\n",
    "def in_hull(p, hull):\n",
    "    \"\"\"\n",
    "    Test if points in `p` are in `hull`\n",
    "\n",
    "    `p` should be a `NxK` coordinates of `N` points in `K` dimensions\n",
    "    `hull` is either a scipy.spatial.Delaunay object or the `MxK` array of the \n",
    "    coordinates of `M` points in `K`dimensions for which Delaunay triangulation\n",
    "    will be computed\n",
    "    \"\"\"\n",
    "    from scipy.spatial import Delaunay\n",
    "    if not isinstance(hull, Delaunay):\n",
    "        hull = Delaunay(hull)\n",
    "\n",
    "    return hull.find_simplex(p) >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over convex hull objects and match points\n",
    "\n",
    "cluster_hull_objs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = parsed_df.copy()\n",
    "\n",
    "df['x_id'] = df['x'] * 100\n",
    "df['x_id'] = df['x_id'].astype(int)\n",
    "df['y_id'] = df['y'] * 100\n",
    "df['y_id'] = df['y_id'].astype(int)\n",
    "\n",
    "# save ids to parsed_df\n",
    "parsed_df = df.copy()\n",
    "\n",
    "df['count'] = 1\n",
    "\n",
    "# get count and angle by unique location\n",
    "df = df.\\\n",
    "    groupby(['x', 'y', 'x_id', 'y_id']).\\\n",
    "    agg({\"count\": np.sum, 'bearing': np.median}).\\\n",
    "    rename(columns={'bearing': 'bearing_median'}).\\\n",
    "    reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cluster_cols = []\n",
    "cluster_keys = list(cluster_hull_dict.keys())\n",
    "cluster_keys.sort()\n",
    "\n",
    "for cluster_hull in cluster_keys:\n",
    "    col_name = \"cluster_{}\".format(str(cluster_hull))\n",
    "    all_cluster_cols.append(col_name)\n",
    "    df[col_name] = in_hull(\n",
    "        p=df[['x', 'y']].to_numpy(),\n",
    "        hull=cluster_hull_dict[cluster_hull])\n",
    "    df.loc[df[col_name]==True, 'seg_dir_lane_cluster'] = str(cluster_hull)\n",
    "    df = df.drop(columns=[col_name])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge id table with name table\n",
    "\n",
    "# use all points - allow duplicate identicals\n",
    "# clustered_df = parsed_df.merge(\n",
    "#     df.drop(columns=['x', 'y']), on=['x_id', 'y_id'])\n",
    "\n",
    "# use only unique points - disallow identicals\n",
    "\n",
    "subcluster_results_df = df.copy()\n",
    "\n",
    "subcluster_results_df['seg_dir_lane_cluster'] = subcluster_results_df['seg_dir_lane_cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove nan\n",
    "\n",
    "subcluster_results_df = subcluster_results_df[subcluster_results_df['seg_dir_lane_cluster']!='nan']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge seg_dir_lane_cluster with all applicable data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = subcluster_results_df.copy()\n",
    "\n",
    "df['x_id'] = df['x'] * 100\n",
    "df['x_id'] = df['x_id'].astype(int)\n",
    "df['y_id'] = df['y'] * 100\n",
    "df['y_id'] = df['y_id'].astype(int)\n",
    "\n",
    "\n",
    "subcluster_results_df = df[['x_id', 'y_id', 'seg_dir_lane_cluster']].copy()\n",
    "subcluster_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df = parsed_df.merge(\n",
    "    subcluster_results_df, on=['x_id', 'y_id']).\\\n",
    "    sort_values(by='record_id', axis=0)  # make sure record is in order\n",
    "clustered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df.groupby(['seg_dir_lane_cluster']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### End of Clustering Models for Roadway Geometries ###########\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Congestion \"Clusters\" Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each cluster, and time step\n",
    "# cluster queues based on DBSCAN, set a avg speed eligibility ~ 10kph (no tailgating) - if a whole set of points are fast but close, ignore\n",
    "# \n",
    "# find queue length based on furthest point algorithm function - these points are start and end of queues\n",
    "# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define speed threshold\n",
    "speed_threshold = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# create time bin for the clustered df data\n",
    "\n",
    "# calculate time bin based on max and min values, then do every x seconds\n",
    "\n",
    "x_sec_bin = 0.02  # step size - shouldn't be too large, if 0.02, no bin\n",
    "\n",
    "min_time = min(clustered_df['time'])\n",
    "max_time = max(clustered_df['time'])\n",
    "\n",
    "if(x_sec_bin <= 0.02):\n",
    "    clustered_df['time_bin'] = clustered_df['time'].copy()\n",
    "else:\n",
    "    clustered_df['time_bin'] = x_sec_bin * \\\n",
    "        np.round(clustered_df['time']/x_sec_bin, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a quick analysis on the count by time bins\n",
    "\n",
    "clustered_df['count'] = 1\n",
    "\n",
    "cluster_time_df = clustered_df[clustered_df['speed'] < speed_threshold].\\\n",
    "    groupby(['seg_dir_lane_cluster', 'time_bin']).\\\n",
    "    agg({'count': np.sum}).\\\n",
    "    reset_index()\n",
    "\n",
    "# cluster_time_df = cluster_time_df[cluster_time_df['count'] > 1]\n",
    "\n",
    "# for testing, use whole seconds only\n",
    "# wholoe_seconds_only = ~(cluster_time_df['time'].astype(int) < cluster_time_df['time'])\n",
    "# cluster_time_df = cluster_time_df[wholoe_seconds_only]\n",
    "\n",
    "# cluster_time_list = cluster_time_df.\\\n",
    "#     drop(columns=['count']).\\\n",
    "#     to_numpy()\n",
    "\n",
    "# # len(cluster_time_list)\n",
    "# cluster_time_df[cluster_time_df['seg_dir_lane_cluster'] == '7_3']  # ['count'].\n",
    "\n",
    "max(cluster_time_df['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(cluster_time_df['count']) # let's get rid of these groups, they won't have any queues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(clustered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df_eval = clustered_df.merge(cluster_time_df.rename(\n",
    "    columns={'count': 'time_bin_count'}), on=['seg_dir_lane_cluster', 'time_bin'])\n",
    "\n",
    "# exclude cluster and time points with no more than 1 sample\n",
    "clustered_df_eval = clustered_df_eval[clustered_df_eval['time_bin_count'] > 1]\n",
    "\n",
    "# exclude points that are moving faster than 10 kph\n",
    "clustered_df_eval = clustered_df_eval[clustered_df_eval['speed'] < speed_threshold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df_eval.groupby(['seg_dir_lane_cluster']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [x for i, x in df.groupby(level=0, sort=False)]\n",
    "\n",
    "cluster_df_eval_list = [x for i, x in clustered_df_eval.groupby(['seg_dir_lane_cluster', 'time_bin'], sort=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_df_eval_list[0]['time']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# congestion_parameter = {\n",
    "#     'metric': 'euclidean',\n",
    "#     'min_cluster_size': 2,\n",
    "#     'min_samples': 2,\n",
    "#     'cluster_selection_epsilon': 15\n",
    "# }\n",
    "\n",
    "# df = cluster_hdbscan(df=cluster_df_eval_list[96],\n",
    "#                      parameters=congestion_parameter,\n",
    "#                      feature_names=['x', 'y'],\n",
    "#                      label_name='cong_flag',\n",
    "#                      verbose=False)\n",
    "\n",
    "congestion_parameter = {\n",
    "    'min_samples': 2,\n",
    "    'cluster_selection_epsilon': 20\n",
    "}\n",
    "\n",
    "df = cluster_dbscan(df=cluster_df_eval_list[96],\n",
    "                    parameters=congestion_parameter,\n",
    "                    feature_names=['x', 'y'],\n",
    "                    label_name='cong_flag')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col='cong_flag', col_wrap=5, height=4)\n",
    "g = g.map(plt.scatter, 'x', 'y', s=10, marker='.')#, edgecolor=\"w\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Congestion Clustering with HDBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run clustering for congestion for lanes\n",
    "\n",
    "# congestion_parameter = {\n",
    "#     'metric': 'euclidean',\n",
    "#     'min_cluster_size': 2,\n",
    "#     'min_samples': 2,\n",
    "#     'cluster_selection_epsilon': 15\n",
    "# }\n",
    "\n",
    "# cong_cluster_df_eval_list = [(cluster_hdbscan(df=df,\n",
    "#                                               parameters=congestion_parameter,\n",
    "#                                               feature_names=['x', 'y'],\n",
    "#                                               label_name='cong_flag',\n",
    "#                                               verbose=False)\n",
    "#                               )\n",
    "#                              for df in cluster_df_eval_list]\n",
    "\n",
    "\n",
    "congestion_parameter = {\n",
    "    'min_samples': 2,\n",
    "    'cluster_selection_epsilon': 20\n",
    "}\n",
    "\n",
    "cong_cluster_df_eval_list = [(cluster_dbscan(df=df,\n",
    "                                             parameters=congestion_parameter,\n",
    "                                             feature_names=['x', 'y'],\n",
    "                                             label_name='cong_flag',\n",
    "                                             verbose=False)\n",
    "                              )\n",
    "                             for df in cluster_df_eval_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visual checks\n",
    "\n",
    "g = sns.FacetGrid(cong_cluster_df_eval_list[60], col='cong_flag', col_wrap=5, height=4)\n",
    "g = g.map(plt.scatter, 'x', 'y', s=10, marker='.')#, edgecolor=\"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine results from clustering\n",
    "\n",
    "cong_cluster_df_result = pd.concat(cong_cluster_df_eval_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all outliers (not dense enough to qualify as queues)\n",
    "\n",
    "cong_cluster_df_result = cong_cluster_df_result[cong_cluster_df_result['cong_flag'] != -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_cluster_df_result.groupby(['seg_dir_lane_cluster']).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cong_cluster_df_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build intermediate dataframe\n",
    "# https://stackoverflow.com/questions/35234012/python-pandas-merge-two-tables-without-keys-multiply-2-dataframes-with-broadc\n",
    "# calculate Euclidean distance\n",
    "# more resources for more complex examples: https://kanoki.org/2019/12/27/how-to-calculate-distance-in-python-and-pandas-using-scipy-spatial-and-distance-functions/\n",
    "def e_dist(x1, x2, y1, y2):\n",
    "    return np.sqrt((x1-x2) ** 2+(y1-y2) ** 2)\n",
    "\n",
    "\n",
    "def getQueue(df):\n",
    "    \"\"\"This function requires the dataframe input to be groupped into appropriate clusters\"\"\"\n",
    "\n",
    "    df1 = df.copy()\n",
    "\n",
    "    df1['tmp'] = 1\n",
    "    \n",
    "    if len(df1) > 0:\n",
    "        # put data in list\n",
    "\n",
    "        df_dist = pd.merge(df1, df1, on=['tmp'], suffixes=(\n",
    "            '_1', '_2'))\n",
    "\n",
    "        df_dist['dist'] = e_dist(\n",
    "            x1=df_dist['x_1'],\n",
    "            x2=df_dist['x_2'],\n",
    "            y1=df_dist['y_1'],\n",
    "            y2=df_dist['y_2'])\n",
    "\n",
    "        # get maximum distance in each group\n",
    "    #     idx = df_dist.groupby(['cong_flag_1'])['dist'].transform(max) == df_dist['dist']\n",
    "        idx = df_dist['dist'].max() == df_dist['dist']\n",
    "        \n",
    "        # keeping the first is good enough - idx will return 2 copy\n",
    "        result = df_dist[idx].iloc[0]\n",
    "\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# queue_calc_eval_list = [x for i, x in cong_cluster_df_result.groupby(\n",
    "#     ['seg_dir_lane_cluster', 'time_bin', 'cong_flag'], sort=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # test\n",
    "# getQueue(queue_calc_eval_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_queue_calc_result = cong_cluster_df_result.groupby(\n",
    "    ['seg_dir_lane_cluster', 'time_bin', 'cong_flag']).apply(lambda grp: getQueue(grp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_queue_calc_result_final = ct_queue_calc_result.\\\n",
    "    reset_index()\\\n",
    "    [['seg_dir_lane_cluster', 'time_bin', 'cong_flag', 'record_id_1',\n",
    "        'record_id_2', 'lat_1', 'lon_1', 'lat_2', 'lon_2', 'dist']]\n",
    "\n",
    "# for each row is a recorded queue\n",
    "# where \n",
    "# seg_dir_lane_cluster is the road segment direction and lane cluster group\n",
    "# time_bin is the time stamp\n",
    "# cong_flag is the queue number\n",
    "# record_id_1 record id of the track and time of the start of the queue\n",
    "# lat_1 is the latitude of start of the queue\n",
    "# lon_1 is the longitude of start of the queue\n",
    "# record_id_2 record id of the track and time of the end of the queue\n",
    "# lat_2 is the latitude of end of the queue\n",
    "# lon_2 is the longitude of end of the queue\n",
    "# dist is the queue length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save all the queues\n",
    "ct_queue_calc_result_final.to_csv('uas4t_tl_team_queue-revised.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Report maximum queue for each cluster\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each cluster, find the max queue length\n",
    "# then report\n",
    "## i. Maximum length of queue, \n",
    "## ii. Lane the maximum length occurred, \n",
    "## iii. Coordinates of the start and end of the maximum queue, \n",
    "## iv. Timestamp of the maximum queue occurrence, and v. whether, when and where a spillback is formed (when applicable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_queue_calc_result_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max queue by cluster\n",
    "max_dist = ct_queue_calc_result_final.\\\n",
    "    groupby(['seg_dir_lane_cluster']).\\\n",
    "    agg({'dist': np.max}).\\\n",
    "    rename(columns={'dist': 'max_queue_length'}).\\\n",
    "    reset_index()\n",
    "\n",
    "max_queue_df = ct_queue_calc_result_final.merge(max_dist, on='seg_dir_lane_cluster')\n",
    "max_queue_df = max_queue_df[max_queue_df['max_queue_length'] == max_queue_df['dist']]\n",
    "\n",
    "max_queue_df.to_csv('uas4t_tl_team_results-revised.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(max_queue_df.columns)\n",
    "\n",
    "# for each row is a recorded max queue per cluster over one or more time interval\n",
    "# where \n",
    "# seg_dir_lane_cluster is the road segment direction cluster group\n",
    "# time_bin is the time stamp\n",
    "# cong_flag is the queue number\n",
    "# record_id_1 record id of the track and time of the start of the queue\n",
    "# lat_1 is the latitude of start of the queue\n",
    "# lon_1 is the longitude of start of the queue\n",
    "# record_id_2 record id of the track and time of the end of the queue\n",
    "# lat_2 is the latitude of end of the queue\n",
    "# lon_2 is the longitude of end of the queue\n",
    "# max_queue_length is the maxmimum queue length (equals to dist, aka queue length)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vnh9lVzoDKBw"
   },
   "source": [
    "# End of Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "d_imAkofDLv8"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyONxkIoD6wUGXeMHfkLs1qj",
   "collapsed_sections": [],
   "mount_file_id": "1b7meJlCGMbE3nQyrVRkjyCMMCtNzkLIA",
   "name": "2020_UAS4T_BW.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "273.141px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
